# 因子化触发器攻击 - 最终优化配置
# 基于三次训练经验的最优方案

# ============= 基础配置 =============
dataset: 'cifar10'
seed: 42                       # 改变随机种子
epochs: 200                    # 增加到200轮
poison_epochs: 200
poison_start_epoch: 0

# ============= 联邦学习配置 =============
num_total_participants: 100
num_sampled_participants: 10
num_adversaries: 3             # 3个攻击者（折中方案）
sample_method: 'random'
dirichlet_alpha: 0.9

# ============= 训练配置（关键优化）=============
lr: 0.005                      # ✓ 降低学习率（从0.01到0.005）
                               # 原因：更稳定的收敛
target_lr: 0.01
lr_method: 'step'              # ✓ 改用阶梯衰减
lr_decay_epochs: [100, 150]    # 在epoch 100和150降低学习率
lr_decay_factor: 0.1

momentum: 0.9
decay: 0.001                   # ✓ 增加权重衰减（从0.0005到0.001）
batch_size: 64
test_batch_size: 1024
retrain_times: 3               # ✓ 增加良性训练（从2到3）
attacker_retrain_times: 2      # ✓ 攻击者训练2次

# ============= 聚合方法配置 =============
agg_method: 'avg'
clip_factor: 1

# ============= 核心配置（基于第2次的成功经验）=============
attacker_method: 'factorized'

k_of_m_k: 2
k_of_m_m: 3

# 动态强度（微调）
initial_intensity: 0.12        # 0.1 → 0.12（略微提高）
final_intensity: 0.55          # 0.5 → 0.55（略微提高）

# 任务分离（微调到最优点）
task_separation_weight: 0.35   # 在0.3和0.4之间找平衡
                               # 65%主任务，35%后门

# 轮换策略
rotation_strategy: 'adversary_specific'
rotation_frequency: 1

# 攻击目标（微调）
target_class: 2
bkd_ratio: 0.18               # 15% → 18%（折中）

# 评估频率
eval_freq: 10

# ============= 可视化配置 =============
visualization:
  enabled: true
  save_interval: 10
  save_dir: './visualizations_final'

# ============= 输出配置 =============
environment_name: 'factorized_final'
save_model: true
save_on_epochs: [50, 100, 150, 200]
results_dir: './results/factorized_final'

